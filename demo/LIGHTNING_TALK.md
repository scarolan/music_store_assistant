# üé∏ "Demystifying AI Observability" - Lightning Talk
**10 Minutes | Music Store Assistant Demo | Grafana Cloud**

---

## üéØ Talk Structure (7 mins content + 3 mins Q&A)

### Slide 1: Title Slide (0:00-0:30)
**Visual**: Music store logo + Grafana logo
**Title**: "Your AI is Talking to Itself... And You Have No Idea What It's Saying"
**Subtitle**: "A 7-Minute Journey into AI Observability"

**Talk Track**:
> "Hey folks! Quick question - how many of you have built or are building with LLMs? [pause for hands] Awesome. Now keep your hand up if you know EXACTLY how much each conversation costs you. [pause - most hands drop] Yeah, that's what I thought. Today I'm going to show you why your AI agents are basically teenagers - they're expensive, unpredictable, and they need constant supervision. But unlike teenagers, we can actually observe what they're doing."

---

### Slide 2: The Problem (0:30-1:30)
**Visual**: Split screen - "What You Built" vs "What's Actually Happening"
- Left: Simple chatbot flow diagram
- Right: Chaotic spaghetti of agent calls, tool invocations, retries

**Talk Track**:
> "So you built an AI agent. Congrats! You used LangGraph, LangChain, maybe some Claude or GPT-4. You deployed it. Users love it. Then your AWS bill shows up and it's... [dramatic pause] ...not great. You have three questions:
>
> 1. Why did that conversation cost $12 in API calls?
> 2. Why is my agent calling the database 47 times for one user question?
> 3. Is my system prompt even working, or is my agent just hallucinating its way through life?
>
> Traditional observability doesn't help here. You can see HTTP requests, sure. But you CAN'T see what your agents are thinking, which tools they're calling, or why they made 6 LLM calls when 2 would've been fine."

---

### Slide 3: The Demo App (1:30-2:00)
**Visual**: Architecture diagram of Music Store Assistant
- Supervisor Router ‚Üí Music Expert / Support Rep
- Show tool nodes, HITL gate, database

**Talk Track**:
> "Let me show you a real multi-agent system. This is Algorhythm - a music store customer support bot. It's got:
> - A Supervisor that routes requests
> - A Music Expert that's creative and knows the catalog
> - A Support Rep that handles refunds with human-in-the-loop approval
>
> Classic agentic architecture. It's running on localhost right now with real traffic we generated earlier. Let's see what's actually happening under the hood."

---

### Slide 4: Live Demo Part 1 - The Happy Path (2:00-3:30)
**Visual**: Screen share split - Customer UI + Grafana Dashboard

**Talk Track**:
> "Let me fire up the customer interface and ask something simple: 'What albums does Pink Floyd have?'
>
> [Type query, hit send]
>
> Watch the dashboard light up. Look at this trace - we can see:
>
> 1. **The Supervisor** decided this is a music query - routed to Music Expert
> 2. **The Music Expert** made ONE LLM call - you can see the prompt tokens (X tokens) and completion tokens (Y tokens)
> 3. **Tool usage** - it called `get_albums_by_artist` with artist='Pink Floyd'
> 4. **Total latency** - 1.2 seconds end-to-end
> 5. **Cost attribution** - this conversation cost us about $0.0003
>
> That's the happy path. But things get interesting when agents misbehave."

---

### Slide 5: Live Demo Part 2 - The Chaos (3:30-5:00)
**Visual**: Grafana dashboard showing a long conversation trace

**Talk Track**:
> "Now let's look at one of those long conversations we generated - this customer asked 6 questions in a row about jazz artists. Watch what happens to the prompt tokens.
>
> [Pull up long-convo-1 trace]
>
> See how the prompt tokens GROW with each turn? Turn 1: 500 tokens. Turn 6: 2,800 tokens. Why? Because LangGraph is including the full conversation history every time. That's 5.6x token growth.
>
> Now here's the million-dollar question - literally: If you have 10,000 customers having 6-turn conversations, and you don't know this is happening, you just 5x'd your API costs without realizing it.
>
> But it gets better. Look at this trace from earlier - [pull up a failed query] - zero response characters. The LLM hallucinated a tool call that doesn't exist, errored out, and we burned tokens for NOTHING. Without tracing, you'd never know this happened."

---

### Slide 6: The Money Shot - What You Actually Learn (5:00-6:00)
**Visual**: Dashboard with key metrics highlighted
- Token usage over time
- Tool call patterns
- Error rates by agent
- P95 latency by conversation type

**Talk Track**:
> "So what does AI observability actually give you? Four things:
>
> 1. **Cost Attribution** - You can see EXACTLY which conversations cost what. Music queries average $0.0004. Support queries with refunds? $0.002. That's 5x more expensive.
>
> 2. **Behavioral Patterns** - You can see when agents are calling tools unnecessarily. One customer asked 'What's your refund policy?' and the agent called the database. Why? It's in the system prompt. That's wasted tokens.
>
> 3. **Prompt Engineering Validation** - You can see if your prompts are working. If the Supervisor is routing 30% of music questions to the Support Rep, your routing prompt is broken.
>
> 4. **Debugging** - When something fails, you don't just see 'HTTP 500'. You see: 'Music Expert called tool X with parameter Y, LLM response was Z, tool execution failed because database timeout'. That's ACTIONABLE."

---

### Slide 7: How This Actually Works (6:00-6:45)
**Visual**: Simple architecture diagram
- App ‚Üí OpenTelemetry SDK ‚Üí Grafana Cloud
- OpenInference auto-instrumentation layer

**Talk Track**:
> "Quick implementation note - this isn't magic. It's three things:
>
> 1. **OpenTelemetry SDK** - industry standard observability
> 2. **OpenInference instrumentation** - LangChain-specific wrapper that understands agents, tools, prompts
> 3. **Grafana Cloud Tempo** - stores and visualizes the traces
>
> Total setup time? About 30 minutes. Add 5 lines of code to your app. You're done. Now you can see everything."

---

### Slide 8: The Takeaway (6:45-7:30)
**Visual**: Three key points with icons
- üí∞ Cost Visibility
- üîç Behavioral Insights
- üêõ Faster Debugging

**Talk Track**:
> "Here's my hot take: If you're building AI agents without observability, you're flying blind. You don't know:
> - What they're actually doing
> - How much they're costing
> - Why they fail
> - If your prompts work
>
> And in production, that's terrifying. Your AI is making decisions, spending money, talking to customers... and you're just HOPING it's working.
>
> Observability isn't optional anymore. It's table stakes. Especially when you're burning dollars on API calls.
>
> The good news? It's not hard. OpenTelemetry + OpenInference + Grafana Cloud. Done. You're welcome."

---

### Slide 9: Call to Action (7:30-8:00)
**Visual**: QR code + GitHub repo link + Grafana Cloud trial link

**Talk Track**:
> "If you want to try this yourself - everything I showed you is open source. Scan this QR code, it'll take you to the GitHub repo. The whole app, the OTEL config, even the traffic generators I used.
>
> And if you want to try Grafana Cloud, there's a free tier - scan the second QR code. You can have this running in 30 minutes.
>
> Alright, I've got 2 minutes for questions. Hit me."

---

## üé¨ Demo Preparation Checklist

### Before the Talk:
- [ ] Demo running: `./start_demo.sh` (server + continuous traffic)
- [ ] Grafana dashboard open in browser tab
- [ ] Customer UI open in browser tab
- [ ] Continuous traffic generating: `tail -f /tmp/continuous-traffic.log`
- [ ] Pre-identify 2-3 good trace examples:
  - A clean "happy path" single query
  - A long multi-turn conversation (long-convo-1)
  - A failed/error trace with 0 response chars
- [ ] Test screen sharing - make sure fonts are readable
- [ ] Have backup screenshots in case demo gods are angry

### During the Talk:
- **0:00-2:00**: Slides only, no demo yet (build anticipation)
- **2:00-5:00**: Live demo - customer UI + Grafana side by side
- **5:00-7:30**: Back to slides for insights + takeaways
- **7:30-10:00**: Q&A

### Pro Tips:
- Practice saying "OpenInference" without tripping over it
- Have the Grafana query ready: `{service.name="music-store-assistant"}`
- If demo breaks, pivot to: "And THIS is why we need observability - because demos fail"
- Keep energy HIGH - this is a lightning talk, not a technical deep dive

---

## üé§ Backup Q&A Answers

**Q: "Does this work with non-LangChain frameworks?"**
A: "Great question! OpenInference supports LangChain, LlamaIndex, and DSPy out of the box. For custom frameworks, you can use raw OpenTelemetry spans - it's just more manual. But the 80/20 rule applies - most people are using LangChain/LangGraph."

**Q: "What about cost tracking for other providers like Anthropic or Google?"**
A: "OpenInference captures token counts from ANY provider that exposes them in the response. So yes - Claude, Gemini, DeepSeek - all tracked. The math changes per provider's pricing, but the token counts are universal."

**Q: "Isn't this data sensitive? Are you logging prompts?"**
A: "Amazing question. In this demo, yes - but in production you'd use attribute filtering to DROP sensitive data. We've got a blog post on this - you can strip PII, customer data, whatever. You keep the metadata (token counts, latencies) but drop the actual prompt text."

**Q: "How much does Grafana Cloud cost for this?"**
A: "Free tier covers up to 50GB of traces per month. For most small-to-medium AI apps, that's plenty. When you outgrow it, you're probably making enough money to afford the paid tier. But honestly, the cost of observability is TINY compared to the cost of wasted LLM calls."

**Q: "Can I use this with streaming responses?"**
A: "Yes! Streaming works fine - OpenInference captures the spans when the stream completes. You'll see the full token counts and latency for the entire streamed response."

---

## üé® Slide Design Notes

### Color Palette:
- Primary: Grafana Orange (#FF8C00)
- Secondary: Deep Purple (for AI/agent theme)
- Accent: Electric Blue (for data/metrics)
- Background: Dark mode (looks better for demos)

### Fonts:
- Headers: Bold, sans-serif (Helvetica/Arial)
- Body: Clean, readable sans-serif
- Code snippets: Monospace (Courier/Monaco)

### Visual Style:
- Minimalist - not too busy
- High contrast for readability
- Use icons/emojis for visual interest
- Live demo should be LARGE (50%+ of screen)

---

## üìä Key Metrics to Highlight

Pull these from your actual dashboard during the demo:

- **Total conversations**: 95 (90 short + 5 long)
- **Total API requests**: 176+
- **Average cost per query**: $0.0003-0.002
- **Token growth in long convos**: 5.6x from turn 1 to turn 6
- **Error rate**: ~17% (use this to show why observability matters!)
- **Latency P95**: ~2-3 seconds for complex queries
- **Most expensive conversation**: Pull the longest one from long-convo traces

---

**Last Updated**: 2026-02-10
**Duration**: 10 minutes (7 min talk + 3 min Q&A)
**Difficulty**: üå∂Ô∏èüå∂Ô∏è (Medium - requires live demo confidence)
